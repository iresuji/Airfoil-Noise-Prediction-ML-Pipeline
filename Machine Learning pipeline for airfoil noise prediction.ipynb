{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97b4b553",
   "metadata": {},
   "source": [
    "# Build an ML Pipeline for Airfoil noise prediction\n",
    "\n",
    "\n",
    "## Scenario\n",
    "You are a data engineer at an aeronautics consulting company. Your company prides itself in being able to efficiently design airfoils for use in planes and sports cars. Data scientists in your office need to work with different algorithms and data in different formats. While they are good at Machine Learning, they count on you to be able to do ETL jobs and build ML pipelines. In this project you will use the modified version of the NASA Airfoil Self Noise dataset. You will clean this dataset, by dropping the duplicate rows, and removing the rows with null values. You will create an ML pipe line to create a model that will predict the SoundLevel based on all the other columns. You will evaluate the model and towards the end you will persist the model.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "\n",
    "### Part 1 Perform ETL activity\n",
    "Load a csv dataset\n",
    "Remove duplicates if any\n",
    "Drop rows with null values if any\n",
    "Make transformations\n",
    "Store the cleaned data in parquet format\n",
    "### Part 2 Create a Machine Learning Pipeline\n",
    "Create a machine learning pipeline for prediction\n",
    "### Part 3 Evaluate the Model\n",
    "Evaluate the model using relevant metrics\n",
    "### Part 4 Persist the Model\n",
    "Save the model for future production use\n",
    "Load and verify the stored model\n",
    "\n",
    "## Datasets\n",
    "In this lab you will be using dataset(s):\n",
    "\n",
    "The original dataset can be found here NASA airfoil self noise dataset. https://archive.ics.uci.edu/dataset/291/airfoil+self+noise\n",
    "\n",
    "This dataset is licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) license."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafd9009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install the required libraries\n",
    "\n",
    "!pip install pyspark==3.1.2 -q\n",
    "!pip install findspark -q\n",
    "!pip install pandas==1.3.4\n",
    "!pip install scikit-learn==1.0.2\n",
    "!pip install numpy==1.21.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d8bf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FindSpark simplifies the process of using Apache Spark with Python\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90822233",
   "metadata": {},
   "source": [
    "## PART 1 - PERFORM ETL ACTIVITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d68451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db9f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Airfoil noise prediction').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643d9a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the csv file into a dataframe\n",
    "\n",
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/datasets/NASA_airfoil_noise_raw.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cbb7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into the spark dataframe\n",
    "\n",
    "df = spark.read.csv('NASA_airfoil_noise_raw.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac910515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the top 5 rows of the dataset\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c486544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the total number of rows in the dataset\n",
    "rowcount1 =df.count()\n",
    "print(rowcount1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c746fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop all the duplicate rows from the dataset\n",
    "df2 = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c8e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the total number of rows in the dataset\n",
    "\n",
    "rowcount2 = df.dropDuplicates().count()\n",
    "print(rowcount2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e830dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop all the rows that contain null values from the dataset and print the total number of rows in the dataset\n",
    "df_cleaned = df.dropDuplicates().dropna()\n",
    "rowcount3 = df_cleaned.count()\n",
    "print(rowcount3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef3067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the column \"SoundLevel\" to \"SoundLevelDecibels\"\n",
    "df_renamed = df_cleaned.withColumnRenamed(\"SoundLevel\", \"SoundLevelDecibels\")\n",
    "df_renamed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141f863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the dataframe in parquet format, name the file as \"NASA_airfoil_noise_cleaned.parquet\"\n",
    "\n",
    "df_renamed.write.mode(\"overwrite\").parquet(\"NASA_airfoil_noise_cleaned.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4119c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!!ls -l NASA_airfoil_noise_cleaned.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6fdf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the dataset rows \n",
    "\n",
    "print(\"Part 1 - Evaluation\")\n",
    "\n",
    "print(\"Total rows = \", rowcount1)\n",
    "print(\"Total rows after dropping duplicate rows = \", rowcount2)\n",
    "print(\"Total rows after dropping duplicate rows and rows with null values = \", rowcount3)\n",
    "print(\"New column name = \", df.columns[-1] if df else None)\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"NASA_airfoil_noise_cleaned.parquet exists :\", os.path.isdir(\"NASA_airfoil_noise_cleaned.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ec65d1",
   "metadata": {},
   "source": [
    " ## PART 2 - CREATE A MACHINE LEARNING PIPELINE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3e4322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data from \"NASA_airfoil_noise_cleaned.parquet\" into a dataframe\n",
    "df_loaded = spark.read.parquet(\"NASA_airfoil_noise_cleaned.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451c28bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print total number of rows in dataset\n",
    "rowcount4 = df_loaded.count()\n",
    "print(rowcount4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9236e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1917a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the vector assembler pipeline stage\n",
    "#Stage 1 - Assemble the input columns into a single column \"features\". Use all the columns except SoundLevelDecibels as input features.\n",
    "\n",
    "input_columns = [col for col in df_loaded.columns if col != 'SoundLevelDecibels']\n",
    "assembler = VectorAssembler(inputCols=input_columns, outputCol='features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd45eb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the \"features\" using standard scaler and store in \"scaledFeatures\" column\n",
    "\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures', withStd=True, withMean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6287b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a LinearRegression stage to predict \"SoundLevelDecibels\"\n",
    "\n",
    "lr = LinearRegression(featuresCol='scaledFeatures', labelCol='SoundLevelDecibels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66cc5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline with all three stages\n",
    "pipeline = Pipeline(stages=[assembler, scaler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702ae1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and testing with a 70:30 split. Set the value of seed to 42\n",
    "\n",
    "# Set the seed value\n",
    "seed_value = 42\n",
    "\n",
    "# Split the data into training and testing sets with a 70:30 split\n",
    "train_data, test_data = df_loaded.randomSplit([0.7, 0.3], seed=seed_value)\n",
    "\n",
    "# Display the count of rows in each set\n",
    "print(\"Training set count: \", train_data.count())\n",
    "print(\"Testing set count: \", test_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f520cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the pipeline\n",
    "pipelineModel = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a059ad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Part 2 - Evaluation\")\n",
    "print(\"Total rows = \", rowcount4)\n",
    "ps = [str(x).split(\"_\")[0] for x in pipeline.getStages()]\n",
    "\n",
    "print(\"Pipeline Stage 1 = \", ps[0])\n",
    "print(\"Pipeline Stage 2 = \", ps[1])\n",
    "print(\"Pipeline Stage 3 = \", ps[2])\n",
    "\n",
    "print(\"Label column = \", lr.getLabelCol())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75c110d",
   "metadata": {},
   "source": [
    "## PART 3 - EVALUATE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ccdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing data\n",
    "predictions = pipelineModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe8462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the MSE(mean squared error)\n",
    "\n",
    "# Create a RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(labelCol='SoundLevelDecibels', predictionCol='prediction', metricName='mse')\n",
    "\n",
    "# Calculate the MSE on the testing data\n",
    "mse = evaluator.evaluate(predictions)\n",
    "\n",
    "# Print the MSE\n",
    "print(\"Mean Squared Error (MSE):\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a061319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the MAE(Mean absolute error)\n",
    "\n",
    "\n",
    "# Create a RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(labelCol='SoundLevelDecibels', predictionCol='prediction', metricName='mae')\n",
    "\n",
    "# Calculate the MAE on the testing data\n",
    "mae = evaluator.evaluate(predictions)\n",
    "\n",
    "# Print the MAE\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2008345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the R-Squared(R2)\n",
    "\n",
    "# Create a RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(labelCol='SoundLevelDecibels', predictionCol='prediction', metricName='r2')\n",
    "\n",
    "# Calculate the R2 on the testing data\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "\n",
    "# Print the R2\n",
    "print(\"R-Squared (R2):\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3685be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Part 3 - Evaluation\")\n",
    "\n",
    "print(\"Mean Squared Error = \", round(mse,2))\n",
    "print(\"Mean Absolute Error = \", round(mae,2))\n",
    "print(\"R Squared = \", round(r2,2))\n",
    "\n",
    "lrModel = pipelineModel.stages[-1]\n",
    "\n",
    "print(\"Intercept = \", round(lrModel.intercept,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0d3791",
   "metadata": {},
   "source": [
    "## PART 4 - PERSIST THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8680dfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "\n",
    "# Specify the path to save the model\n",
    "model_path = \"Final_Project\"\n",
    "\n",
    "# Save the pipeline model to the specified path\n",
    "pipelineModel.write().overwrite().save(model_path)\n",
    "\n",
    "# Print a confirmation message\n",
    "print(f\"Pipeline model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba095766",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model from the path \"Final_Project\"\n",
    "\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# Specify the path where the model is saved\n",
    "loaded_model_path = \"Final_Project\"\n",
    "\n",
    "# Load the pipeline model from the specified path\n",
    "loadedPipelineModel = PipelineModel.load(loaded_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088ca02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions on the test data using the loaded model\n",
    "predictions_loaded_model = loadedPipelineModel.transform(test_data)\n",
    "\n",
    "# Display the predictions\n",
    "predictions_loaded_model.select('SoundLevelDecibels', 'prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd2fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Part 4 - Evaluation\")\n",
    "\n",
    "loadedmodel = loadedPipelineModel.stages[-1]\n",
    "totalstages = len(loadedPipelineModel.stages)\n",
    "inputcolumns = loadedPipelineModel.stages[0].getInputCols()\n",
    "\n",
    "print(\"Number of stages in the pipeline = \", totalstages)\n",
    "for i,j in zip(inputcolumns, loadedmodel.coefficients):\n",
    "    print(f\"Coefficient for {i} is {round(j,4)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
